
# Import Data
```{python}
import os
import pandas as pd
# import statsmodels.api as sm

df = pd.read_csv(os.path.join(os.getcwd(),'data','iris.csv'))
print(df.head())

X = df[['Sepal.Length','Sepal.Width','Petal.Length','Petal.Width']]
X['intercept'] = 1
Y = df['Species']
Y = Y.map({'setosa':0,'versicolor':1})

print(Y.unique()) 
print(X.head())
```

# Frequentist Logistic Regression
```{python}
import statsmodels.api as sm

log_reg = sm.Logit(Y,X).fit()
print(log_reg.summary())
```

## Frequestist Logistic Regressions
- What do you observe?
- Do you see any problems?

The data completely seperates the target variable. Meaning there is some rule that would b

# Bayesian Logistic Regression - Vauge Priors
```{python}
import arviz as az
import pymc as pm 


with pm.Model() as model:
    # Beta priors
    beta = pm.Normal('beta', mu=0, sigma=100, shape=X.shape[1])

    # Logit Model
    p = pm.invlogit(pm.math.dot(X, beta))

    # Likelihood of the data
    y_obs = pm.Binomial('y_obs',n=X.shape[0],p=p, observed=Y)

    # Inference
    trace = pm.sample(2000, tune=1000)
    ppc = pm.sample_posterior_predictive(trace)

# Analyzing results
print(X.columns)
print(pm.summary(trace))

```

## Trace
```{python}
import matplotlib.pyplot as plt
plt.figure()
az.plot_trace(trace, figsize=(10, 7));
plt.show()
```

# Bayesian Logistic Regression - N(0,1) Priors on Betas

```{python}

with pm.Model() as model:
    # Beta priors
    beta = pm.Normal('beta', mu=0, sigma=1, shape=X.shape[1])

    # Logit Model
    p = pm.invlogit(pm.math.dot(X, beta))

    # Likelihood of the data
    y_obs = pm.Binomial('y_obs',n=X.shape[0],p=p, observed=Y)

    # Inference
    trace = pm.sample(2000, tune=1000)
    ppc = pm.sample_posterior_predictive(trace)

# Analyzing results
print(X.columns)
print(pm.summary(trace))
```

## Trace
```{python}
import matplotlib.pyplot as plt
plt.figure()
az.plot_trace(trace, figsize=(10, 7));
plt.show()
```

# Comparison of the Three Methods
## Variables
beta[0] = Sepal Length
beta[1] = Sepal Width
beta[2] = Petal Length
beta[3] = Petal Width
beta[4] = Intercept

## Frequentist Logistic Regression
Perfect Seperation and thus failure to converge.
================================================================================
                   coef    std err          z      P>|z|      [0.025      0.975]
--------------------------------------------------------------------------------
Sepal.Length    -4.1173   3316.583     -0.001      0.999   -6504.500    6496.265
Sepal.Width     -8.9814   1815.027     -0.005      0.996   -3566.370    3548.407
Petal.Length     4.4103   1631.257      0.003      0.998   -3192.795    3201.615
Petal.Width     33.8138   5245.781      0.006      0.995   -1.02e+04    1.03e+04
intercept        9.6813    1.2e+04      0.001      0.999   -2.35e+04    2.35e+04
================================================================================

## Vauge Priors
          mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  ess_tail  r_hat
beta[0] -0.237  0.422  -1.013    0.579      0.008    0.006    3001.0    3615.0    1.0
beta[1] -0.945  0.487  -1.832   -0.022      0.008    0.006    3689.0    3718.0    1.0
beta[2]  0.692  0.596  -0.453    1.775      0.012    0.009    2654.0    2698.0    1.0
beta[3]  1.065  1.253  -1.225    3.448      0.023    0.016    3018.0    3344.0    1.0
beta[4] -5.077  1.843  -8.544   -1.610      0.031    0.022    3597.0    4103.0    1.0

## N(0,1) Priors
          mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  ess_tail  r_hat
beta[0] -0.783  0.314  -1.361   -0.187      0.005    0.004    3384.0    3732.0    1.0
beta[1] -1.164  0.376  -1.870   -0.462      0.006    0.004    4341.0    4561.0    1.0
beta[2]  0.821  0.375   0.108    1.507      0.007    0.005    3227.0    4028.0    1.0
beta[3]  0.770  0.730  -0.610    2.139      0.011    0.009    4053.0    4595.0    1.0
beta[4] -1.382  0.875  -3.006    0.289      0.013    0.010    4736.0    4762.0    1.0


### Comparison
1) Results in complete seperation - leading to unreliable coefficient estimates. A solution would collect more data if possible, or choose a different model.
    If we ignore the complete seperation and try to interpret. The Intercept
2)
3) HDI of beta[0] is all negative, meaning shorter Sepeal legth is more likely vericoler. Similarly for

# Why the Bayesian Solutions are more meaningful?
The Bayesian solutions converge and give us a more meaningful indication of what legnths/widths each species can take on.
For example beta[3] haveing a hdi in positive/negative indicates longer petals are more likely to be versicolor, while it could be seretonia.
Similarly for a coefficient with HDI CS in full negatives tells us this variable tends to indicate the seratonia species.

Convert

### Converting Log odds to probabilities
```{python}
import numpy as np
vague_p_logOdds = {
 'b_0': -0.237,
 'b_1':-0.945,
 'b_2':0.692,
 'b_3':1.065,
 'b_4':-5.077
 }

norm_p_logOdds = {
 'b_0': -0.783,
 'b_1':-1.164,
 'b_2':0.821,
 'b_3':0.770,
 'b_4':-1.382
 }

convert_to_odds_ratio = lambda x: np.exp(x)

vague_p = {k: convert_to_odds_ratio(v) for k,v in vague_p_logOdds.items()}
norm_p = {k: convert_to_odds_ratio(v) for k,v in norm_p_logOdds.items()}
print('Vague Priors')
print(vague_p)
print('N(0,1) Priors')
print(norm_p)
```
