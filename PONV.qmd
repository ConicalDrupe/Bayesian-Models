
# Overview
PONV = post-operative nausea and vomiting
From a meta analysis of several studies we have rates: \
Nausea 37% \
Vomiting 20% \
for patients undergoing anesthesia.

Main and well understood risk factors are:
- Gender (0-male, 1-female)
- history of motion sickness/PONV (True/False, Sinclair Score respectively)
- Smoking status (0/1)
- Duration of anesthesia

# Q1
Use normal noninformative priors on the beta coefficents and gamma prior on the precision parameter (recipricol of variance)

## Get the Data
```{python}
import os
import pandas as pd

data_dir = os.path.join(os.getcwd(),'data','PONV.csv')
df = pd.read_csv(data_dir)

pd.options.display.max_rows = None
pd.options.display.max_columns= None

print(df.head())
print(df.columns)

data = df[['SinclairScore','Gender','Anaesthesiaduration','Smoking','PONVhist']]
data = data.rename(columns={'SinclairScore':'Sinclair','Gender':'g','Anaesthesiaduration':'ad','Smoking':'s','PONVhist':'h'})
print(data.columns)
print(data.shape)
```


## EDA
See: https://bambinos.github.io/bambi/notebooks/ESCS_multiple_regression.html
```{python}
import matplotlib.pyplot as plt
import arviz as az

plt.figure()
az.plot_pair(data.to_dict("list"),marginals=True,textsize=24)
plt.show()
```


## Heiarchy Model using Non-Informative Priors
- Non-Informative meaning Priors have a large variance, so possible values are quite broad. Letting the data speak...
- Normal for Betas
- Gamma for precision (recipricol of variance)

```{python}
import pymc as pm
y = data['Sinclair']
X = data.loc[:,data.columns != 'Sinclair']
X['intercept'] = 1

shape = X.shape[1] #+ 1 # number of beta coefficients

with pm.Model() as model:
    # Hyperprior on tau (precision of beta coefficients)
    tau = pm.Gamma('tau', alpha=0.2, beta=0.2)  # a, b for Gamma prior are hyperparameters
    # Hierarchical prior on beta coefficients
    beta = pm.Normal('beta', mu=0, tau=tau, shape=shape)

    # Linear model
    mu = pm.math.dot(X, beta)

    # Likelihood of the data
    y_obs = pm.Normal('y_obs', mu=mu,tau=tau, observed=y)

    # Inference
    trace = pm.sample(2000, tune=1000)
    ppc = pm.sample_posterior_predictive(trace)

# Analyzing results
print(X.columns)
print(pm.summary(trace))

```


### Checking R-Score
```{python}
y_pred = ppc.posterior_predictive.stack(sample=("chain", "draw"))["y_obs"].values.T
print(az.r2_score(y.to_numpy(), y_pred))

plt.figure()
az.plot_trace(trace, figsize=(10, 7));
plt.show()
```

